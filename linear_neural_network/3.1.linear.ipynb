{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "**Question 1 : **\n",
    "\n",
    "**  Assume that we have some data $ x = [x_1,...,x_n]\\in \\mathbb{R} $. Our goal is to find a constant $ b $ such that $ \\sum_{i=1}^{n} (x_i-b)^2 $ is minimized.**\n",
    "**  Find a analytic solution for the optimal value of $ b $.**\n",
    "\n",
    "let $ f(x) = \\sum_{i=1}^{n} (x_i-b)^2 $\n",
    "\n",
    "let $ \\nabla_x f(x) = 2 \\sum_{i=1}^{n}(x_i - b) $\n",
    "\n",
    "$ 2 \\sum_{i=1}^{n}(x_i - b) = 0 $\n",
    "\n",
    "$ \\sum_{i=1}^{n}x_i = nb $\n",
    "\n",
    "$ b = \\overline{x} $\n",
    "\n",
    "**  How does this problem and its solution relate to the normal distribution?**\n",
    "\n",
    "the solution is one of the two parameters of a normal distribution, $ b = \\mu $. To get the other one, you just have to do $\\sigma^2 = max(x) - min(x) $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Question 2 : **\n",
    "\n",
    "**  Derive the analytic solution to the optimization problem for linear regression with squared error. To keep things simple, you can omit the bias  b  from the problem (we can do this in principled fashion by adding one column to  X  consisting of all ones).**\n",
    "\n",
    "\n",
    "We need to minimize :\n",
    "\n",
    "$ \\left \\| {y−Xw} \\right \\|^2 $\n",
    "\n",
    "Let's find the minimum of the function by zeroing the derivative :\n",
    "\n",
    "$ \\nabla_w\\left\\|{y−Xw}\\right\\|^2 = 0 $\n",
    "\n",
    "Expanding the derivative :\n",
    "\n",
    "$ \\nabla_w\\left\\|{y−Xw}\\right\\|^2 = \\nabla_w(y−Xw)^\\top(y-Xw) $\n",
    "\n",
    "$ \\nabla_w\\left\\|{y−Xw}\\right\\|^2 = \\nabla_w(y^\\top − w^\\top X^\\top)(y-Xw) $\n",
    "\n",
    "$ \\nabla_w\\left\\|{y−Xw}\\right\\|^2 = \\nabla_w(y^\\top y - y^\\top Xw − w^\\top X^\\top y + w^\\top X^\\top X w) $\n",
    "\n",
    "$ \\nabla_w\\left\\|{y−Xw}\\right\\|^2 = \\nabla_w y^\\top y - \\nabla_w y^\\top Xw − \\nabla_w  w^\\top X^\\top y + \\nabla_w  w^\\top X^\\top X w $\n",
    "\n",
    "$ \\nabla_w\\left\\|{y−Xw}\\right\\|^2 = (y^\\top X)^\\top − X^\\top y + (X^\\top X + (X^\\top X)^\\top)w $\n",
    "\n",
    "$ \\nabla_w\\left\\|{y−Xw}\\right\\|^2 = 2X^\\top X w - X^\\top y − X^\\top y $\n",
    "\n",
    "$ \\nabla_w\\left\\|{y−Xw}\\right\\|^2 = 2X^\\top X w - 2X^\\top y$\n",
    "\n",
    "$ \\nabla_w\\left\\|{y−Xw}\\right\\|^2 = 2X^\\top (Xw - y) $\n",
    "\n",
    "We search :\n",
    "\n",
    "$ 2X^\\top (Xw - y) = 0 $\n",
    "\n",
    "$ X^\\top Xw = X^\\top y $\n",
    "\n",
    "$ w = (X^\\top X)^{-1} X^\\top y $\n",
    "\n",
    "** When might this be better than using stochastic gradient descent? When might this method break? **\n",
    "\n",
    "Whenever the whole formula fit in the memory, this should be better than stochastic gradient descent. This will break whenever $ X^\\top X $ is not inversible. (which should not be often luckily)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}